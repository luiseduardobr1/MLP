{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivação da rede MLP (backpropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T16:18:19.484459Z",
     "start_time": "2020-04-24T16:18:19.461458Z"
    }
   },
   "source": [
    "![title](mlp-imagem.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variáveis\n",
    "\n",
    "- $X$: matriz vertical $(n_0 \\times 1)$ com os valores de entrada da rede neural para uma determinada amostrada;\n",
    "\n",
    "\n",
    "- $W_1$, $W_2$ e $W_3$: matrizes de pesos entre as camadas neurais (vide figura) com dimensões $(\\text{neurônios de saída} \\times \\text{neurônios/valores de entrada})$ de modo que suas colunas são preenchidos com os valores dos pesos conectados a cada entrada da rede neural:\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "x1_1 & x2_1 & x3_1\\\\ \n",
    "x1_2 & x2_2 & x3_2\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "- $I_1$, $I_2$ e $I_3$: matriz de entrada ponderada nas camadas neurais;\n",
    "\n",
    "\n",
    "- $Y_1$, $Y_2$ e $Y_3$: matriz de saída das camadas neurais após aplicação da função de ativação;\n",
    "\n",
    "\n",
    "- $d$: matriz dos valores de saída $(m \\times 1)$ desejados da rede neural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de ativação\n",
    "\n",
    "Deve ser contínua e diferenciável em todo seu domínio. \n",
    "\n",
    "- **Sigmóide**:\n",
    "$\\frac{1}{1+e^{-x}}$\n",
    "\n",
    " - vantagens: pequena variação do gradiente, saídas entre 0 e 1 normalizando a saída cada neurônio, previsões claras à medida que se eleva o valor X tendo y os valores 0 ou 1.\n",
    " - desvantagens: Vanishing gradient (quando o gradiente fica tão pequeno que praticamente não altera os valores da matriz de pesos), saídas não centralizadas no zero, sem valores negativos. \n",
    " \n",
    " \n",
    "- **Tangente hiperbólica**:\n",
    "$\\tanh(x)$\n",
    "    - vantagens: mesmas da sigmóide com a vantagem de ter valores negativos, zero e positivos para y\n",
    "    - desvantagens: praticamente as mesmas da sigmóide\n",
    "    \n",
    "    \n",
    "- **ReLu**:\n",
    "$\\max(0, x)$\n",
    "    - vantagens: permite desativar certos neurônios por ter derivada zero para valores negativos de x. \n",
    "    - desvantagens: próximo do zero ou valores negativos há o problema da aprendizagem por backpropagation\n",
    "    \n",
    "    \n",
    "- **Leaky ReLU**:\n",
    "$f(x) = ax, x < 0$,\n",
    "$f(x) = x, x \\geqslant 0$\n",
    "    - vantagens: resolve o problema de aprendizagem nos valores negativos e próximos a zero do ReLu\n",
    "    - desvantagens: pode haver valores inconsistentes quando x < 0.\n",
    "    obs: geralmente $a=0.01$\n",
    "\n",
    "\n",
    "- **Swish**:\n",
    "$\\frac{x}{1+e^{-x}}$\n",
    "    - vantagens: performance melhor que ReLu com mesmo esforço computacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo do erro\n",
    "\n",
    "### Matriz erro na camada de saída\n",
    "$\\text{erro} = d - Y_3 = \\begin{bmatrix}\n",
    "e_1\\\\ \n",
    "e_2\\\\ \n",
    "...\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Com a matriz $\\text{erro}$ de dimensões $(m \\times 1)$ uma vez que há $m$ neurônios na saída da rede.\n",
    "\n",
    "### Parâmetro do erro de desempenho local\n",
    "Para uma determinada amostra:\n",
    "\n",
    "$\n",
    "E = \\frac{e_1^2+e_2^2 + ... + e_m^2}{2}$\n",
    "\n",
    "em que $e_m$ é o erro da $m$-ésima saída da rede neural. \n",
    "\n",
    "### Erro quadrático médio\n",
    "Considerando todas as $p$ amostras, o erro quadrático médio é dado por\n",
    "\n",
    "$E_M = \\frac{(E_1 + E_2 + ... + E_p)}{p}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação do algoritmo forward - MLP\n",
    "\n",
    "1) Para uma dada amostra de valores da matriz de entrada $X$, tem-se que os vetores de entrada da primeira camada neural é dado por:\n",
    "\n",
    "- $I_1 = W_1 \\times X$ \n",
    "\n",
    "Dimensão: $(n_1 \\times 1)$\n",
    "\n",
    "2) Após isso, encontra-se a matriz da saída dos neurônios $(Y_1)$ com relação à camada 1, em que $f(x)$ representa o valor da função de ativação do neurônio:\n",
    "\n",
    "- $Y_1 = f(I_1)$\n",
    "\n",
    "Dimensão: $(n_1 \\times 1)$\n",
    "\n",
    "3) Para a entrada dos neurônios da segunda camada têm-se:\n",
    "\n",
    "- $I_2 = W_2 \\times Y_1$\n",
    "\n",
    "Dimensão: $(n_2 \\times 1)$\n",
    "\n",
    "4) E as saídas da segunda camada:\n",
    "\n",
    "- $Y_2 = f(I_2)$\n",
    "\n",
    "Dimensão: $(n_2 \\times 1)$\n",
    "\n",
    "5) Analogamente para terceira camada:\n",
    "\n",
    "- $I_3 = W_3 \\times Y_2$\n",
    "\n",
    "Dimensão: $(m \\times 1)$\n",
    "\n",
    "- $Y_3 = f(I_3)$\n",
    "\n",
    "Dimensão: $(m \\times 1)$\n",
    "\n",
    "6) Cálculo do erro:\n",
    "\n",
    "- $\\text{erro} = d - Y_3$\n",
    "\n",
    "- erro de desempenho local: $E = \\frac{e_1^2+e_2^2 + ... + e_m^2}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação do algoritmo backforward - MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para camada de saída (3):\n",
    "\n",
    "$\\nabla E_3 = \\frac{\\partial E}{\\partial W_3} = \\frac{\\partial E}{\\partial Y_3}.\\frac{\\partial Y_3}{\\partial I_3}.\\frac{\\partial I_3}{\\partial W_3}$\n",
    "\n",
    "- $\\frac{\\partial E}{\\partial Y_3}$ (desempenho local)$ = \\frac{2}{2}(d - Y_3)(-1) = -(d - Y_3)$\n",
    "\n",
    "- $\\frac{\\partial Y_3}{\\partial I_3} = f'(I_3)$\n",
    "\n",
    "- $\\frac{\\partial I_3}{\\partial W_3} = Y_2^T$\n",
    "\n",
    "Assim, \n",
    "\n",
    "$\\nabla E_3 = (-(d - Y_3)f'(I_3)) \\times Y_2^T$\n",
    "\n",
    "Como o gradiente tem direção crescente e queremos minimizar o erro, o ajuste de pesos da matrix $W_3$ tem que ser na direção oposta do gradiente de modo que:\n",
    "\n",
    "$\\Delta W_3 = -(-(d - Y_3)f'(I_3)) \\times Y_2^T = \\eta \\delta_3 \\times Y_2^T$\n",
    "\n",
    "em que $\\eta$ é a taxa de aprendizagem e $\\delta_3 = (d - Y_3)f'(I_3)$ com dimensão $(m \\times 1)$\n",
    "\n",
    "Finalmente,\n",
    "\n",
    "$W_{3final} = W_{3inicial} + \\eta \\delta_3 \\times Y_2^T$\n",
    "\n",
    "### Para camada escondida (2):\n",
    "\n",
    "$\\nabla E_2 = \\frac{\\partial E}{\\partial W_2} = \\frac{\\partial E}{\\partial Y_2}.\\frac{\\partial Y_2}{\\partial I_2}.\\frac{\\partial I_2}{\\partial W_2}$\n",
    "\n",
    " - $\\frac{\\partial E}{\\partial Y_2} = \\frac{\\partial E}{\\partial I_3} . \\frac{\\partial I_3}{\\partial Y_2} = W_3^T \\times \\frac{\\partial E}{\\partial I_3} = W_3^T \\times (-(d - Y_3).f'(I_3)) = -W_3^T \\times \\delta_3$\n",
    " - $\\frac{\\partial Y_2}{\\partial I_2} = f'(I_2)$\n",
    " - $\\frac{\\partial I_2}{\\partial W_2} = Y_1^T$\n",
    " \n",
    "Então, \n",
    "\n",
    "$-\\nabla E_2 = -\\frac{\\partial E}{\\partial W_2} = -(-W_3^T \\times \\delta_3 . f'(I_2)) \\times Y_1^T$\n",
    "\n",
    "Definindo $\\delta_2 = (W_3^T \\times \\delta_3). f'(I_2)$ com dimensão $(n_2 \\times 1)$, tem-se, analogamente, para a matriz de peso $W_2$:\n",
    "\n",
    "$W_{2final} = W_{2inicial} + \\eta \\delta_2 \\times Y_1^T$\n",
    "\n",
    "### Para camada inicial (1):\n",
    "\n",
    "De forma análoga às camadas anteriores, tem-se:\n",
    "\n",
    "$\\delta_1 = (W_2^T \\times \\delta_2). f'(I_1)$ com dimensão $(n_1 \\times 1)$\n",
    "\n",
    "$W_{1final} = W_{1inicial} + \\eta \\delta_1 \\times X^T$\n",
    "\n",
    "#### OBSERVAÇÃO: As matrizes de pesos devem ser alteradas no mesmo instante, então, $\\delta_1$ é em função do valor antigo de $W_2$ uma vez que tem que ser calculado ANTES que a atualização das matrizes de peso. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalização das equações\n",
    "\n",
    "Sendo $L$ a $L$-ésima camada neural com $W_L$ é a matriz de pesos entre as camadas $L$ e $L+1$, as equações de correção dos pesos é generalizado por:\n",
    "\n",
    "$W_{L_{final}} = W_{L_{inicial}} + \\eta . \\delta_L \\times Y_{L-1}^T$\n",
    "\n",
    "para $L \\geqslant 1$ com $Y_0 = X$ tal que\n",
    "\n",
    "$\\delta_L = (W_{L+1}^T \\times \\delta_{L+1}).f'(I_L)$\n",
    "\n",
    "sendo satisfeito para $L < \\text{camada de saída}$, caso contrário\n",
    "\n",
    "$\\delta_L = (d - Y_L).f'(I_L)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critério de parada da interação\n",
    "\n",
    "A interação termina quando a diferença entre os erros quadráticos médios de duas épocas sucessivas for menor ou igual à precisão desejada:\n",
    "\n",
    "$|E_{M_{atual}} - E_{M_{anterior}}| \\leqslant \\varepsilon$\n",
    "\n",
    "ou então, se preferir, quando o erro quadrático médio atinge um valor desejado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importância do bias\n",
    "\n",
    "Em todo livro de redes neurais, considera-se um *bias*, ou seja, um valor, geralmente unitário, que é conectado as entradas das camadas neurais a partir de um peso. Sua função consiste em deslocar (*shift*) o gráfico resultante da função de ativação escolhido. \n",
    "\n",
    "Para exemplificar, seja uma rede neural com apenas uma entrada e um neurônio (sem bias) com função de ativação $y=x$, sua saída será dada por:\n",
    "\n",
    "$y = f(xw) = xw$\n",
    "\n",
    "Como pode-se perceber, se trata de uma reta que corta a origem no primeiro e terceiro quadrante (para $w>0$) e no segundo e quarto (para $w<0$). Para os valores de $x=1,2,3$ e $d=1,2,3$, respectivamente, poderíamos treinar a rede sem um bias sem maiores problemas, mas quando $x=1,2,3$ e $d=5,4,3$ ?\n",
    "\n",
    "Nesta situação, é preciso que haja um deslocamento no gráfico, no eixo $y$, para satisfazer a equação $y = -d + 6$ e é exatamente nesse ponto que o bias se torna importante. No primeiro exemplo (tanh(x)) fiz um teste sem bias e foi possível convergir a rede, já no exemplo 2 usando a função de ativação *leaky ReLu* não obtive o mesmo sucesso, obtendo erros elevados na ordem de $7.77$. Realizando o mesmo teste com o bias, a *leaky ReLu* teve um desempenho até melhor convergindo rapidamente. Por esse motivo, é importante que sempre seja adicionado o bias nas entradas das camadas neurais e atualizado seu valor como mostrado no exemplo 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo em Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T23:08:44.869750Z",
     "start_time": "2020-04-25T23:08:44.838550Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar todos os output's\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo 1 - tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T04:14:50.111566Z",
     "start_time": "2020-04-27T04:14:49.940556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[3.89973329]\n",
      "490\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Base de dados a ser treinada\n",
    "# Entradas\n",
    "x = pd.DataFrame(\n",
    "\t[[1],\n",
    "\t[2],\n",
    "\t[3]],\n",
    "\tcolumns=['valores x'])\n",
    "\n",
    "# Valor desejado para saída\n",
    "d = pd.DataFrame(\n",
    "\t[[5],\n",
    "\t[4],\n",
    "\t[3]],\n",
    "\tcolumns=['valores desejados'])\n",
    "\n",
    "# Convertendo o dataframe em array e normalizando os valores desejados (para ficar entre 0 e +1)\n",
    "x = x.to_numpy()\n",
    "d = d/(1.05*d.max())\n",
    "d = d.to_numpy()\n",
    "\n",
    "# Derivada de tanh(x) = sech²(x) = 1 - (tanh(x))²\n",
    "def df(x):\n",
    "    y = 1 - np.power(np.tanh(x), 2)\n",
    "    return y\n",
    "\n",
    "#def rede_mlp(n, x, d, net, k, precisao):\n",
    "\n",
    "# *** Construindo a rede de duas camadas ***\n",
    "\n",
    "# número de neurônios na primeira camada\n",
    "net=3\n",
    "# taxa de aprendizagem\n",
    "n = 0.1\n",
    "# precisão do erro quadrático médio\n",
    "precisao=0.01\n",
    "\n",
    "# Inicializar matrizes de pesos com valores aleatórios distintos\n",
    "w1 = np.random.rand(net,len(x[0]))\n",
    "w2 = np.random.rand(1,net)\n",
    "\n",
    "# Erro quadrático médio inicial e número de épocas\n",
    "E_M=1\n",
    "epocas=0\n",
    "\n",
    "while E_M>precisao:\n",
    "    E_M=0\n",
    "    errofinal=0\n",
    "    for i in range(0,len(x)):\n",
    "        \n",
    "        # FOWARD\n",
    "        i1 = np.matmul(w1, x[i].reshape(len(x[i]),1))\n",
    "        y1 = np.tanh(i1)\n",
    "\n",
    "        i2 = np.matmul(w2, y1)\n",
    "        y2 = np.tanh(i2)\n",
    "        \n",
    "        # erro com o valor desejado\n",
    "        erro = d[i].reshape(len(d[i]),1) - y2\n",
    "        \n",
    "        # BACKPROPAGATION\n",
    "        delta_2 = erro*df(i2)\n",
    "        delta_1 = (np.matmul(w2.T, delta_2))*df(i1)\n",
    "        \n",
    "        w2 = w2 + n*(np.matmul(delta_2, y1.reshape(1, net)))\n",
    "        w1 = w1 + n*(np.matmul(delta_1, x[i].reshape(1, len(x[i]))))\n",
    "        \n",
    "        errofinal = errofinal + 0.5*erro**2\n",
    "        \n",
    "    E_M = errofinal/len(x)\n",
    "    epocas+=1\n",
    "    print(E_M)\n",
    "    \n",
    "    \n",
    "def conferir_peso(w1, w2, entrada, fator_multiplicativo):\n",
    "    i1 = np.matmul(w1, np.array(entrada).T)\n",
    "    y1 = np.tanh(i1)\n",
    "    i2 = np.matmul(w2, y1)\n",
    "    y2 = np.tanh(i2)\n",
    "    y2 = y2*fator_multiplicativo\n",
    "    return(y2)\n",
    "\n",
    "# Conferir o número de épocas para a convergência e o valor encontrado para alguma das entradas\n",
    "print('\\n')\n",
    "valor = conferir_peso(w1, w2, [2], 1.05*5)\n",
    "print(valor)\n",
    "print(epocas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo 2 - Leaky ReLu (sem bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T04:14:45.530303Z",
     "start_time": "2020-04-27T04:14:39.258945Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2ea729f7f1a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mw2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mw1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Base de dados a ser treinada\n",
    "x = pd.DataFrame(\n",
    "\t[[1],\n",
    "\t[2],\n",
    "\t[3]],\n",
    "\tcolumns=['valores x'])\n",
    "\n",
    "d = pd.DataFrame(\n",
    "\t[[5],\n",
    "\t[4],\n",
    "\t[3]],\n",
    "\tcolumns=['valores desejados'])\n",
    "\n",
    "# Convertendo o dataframe em array e normalizando os valores desejados para ficar entre 0 e +1.\n",
    "x = x.to_numpy()\n",
    "# d = d/(1.05*d.max())\n",
    "d = d.to_numpy()\n",
    "\n",
    "\n",
    "# Derivada de tanh(x) = sech²(x) = 1 - (tanh(x))²\n",
    "def df(x):\n",
    "    x = np.array(x)\n",
    "    x[x<=0] = 0.01\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def f(x):\n",
    "    return(np.where(x > 0, x, x * 0.01))\n",
    "#     return(np.maximum(x, 0))\n",
    "\n",
    "    \n",
    "\n",
    "#def rede_mlp(n, x, d, net, k, precisao):\n",
    "\n",
    "# Construindo a rede de duas camadas \n",
    "# net = número de neurônios na primeira camada\n",
    "# n = taxa de aprendizagem\n",
    "# precisao = precisão do erro quadrático médio\n",
    "net=3\n",
    "n = 1e-4\n",
    "precisao=0.0001\n",
    "w1 = np.random.rand(net,len(x[0]))\n",
    "w2 = np.random.rand(1,net)\n",
    "E_M=20\n",
    "epocas=0\n",
    "\n",
    "while E_M>precisao:\n",
    "    E_M=0\n",
    "    errofinal=0\n",
    "    for i in range(0,len(x)):\n",
    "        \n",
    "        # FOWARD\n",
    "        i1 = np.matmul(w1, x[i].reshape(len(x[i]),1))\n",
    "        y1 = f(i1)\n",
    "        \n",
    "        \n",
    "\n",
    "        i2 = np.matmul(w2, y1)\n",
    "        y2 = f(i2)\n",
    "        \n",
    "        \n",
    "        # erro com o valor desejado\n",
    "        erro = d[i].reshape(len(d[i]),1) - y2\n",
    "        \n",
    "        \n",
    "        # BACKPROPAGATION\n",
    "        delta_2 = erro*df(i2)\n",
    "        delta_1 = (np.matmul(w2.T, delta_2))*df(i1)\n",
    "        \n",
    "        w2 = w2 + n*(np.matmul(delta_2, y1.reshape(1, net)))\n",
    "        w1 = w1 + n*(np.matmul(delta_1, x[i].reshape(1, len(x[i]))))\n",
    "        \n",
    "          \n",
    "        errofinal = errofinal + 0.5*erro**2\n",
    "        \n",
    "    #E_M = errofinal/len(x)\n",
    "    E_M = errofinal\n",
    "    epocas+=1\n",
    "    print(E_M)\n",
    "    \n",
    "    \n",
    "def conferir_peso(w1, w2, entrada, fator_multiplicativo):\n",
    "    i1 = np.matmul(w1, np.array(entrada).T)\n",
    "    y1 = np.tanh(i1)\n",
    "\n",
    "    i2 = np.matmul(w2, y1)\n",
    "    y2 = np.tanh(i2)\n",
    "    y2 = y2*fator_multiplicativo\n",
    "    return(y2)\n",
    "\n",
    "#1.05*5\n",
    "valor = conferir_peso(w1, w2, [2], 1)\n",
    "valor\n",
    "print(epocas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo 3 - Leaky ReLu (com bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T04:14:36.451784Z",
     "start_time": "2020-04-27T04:14:35.921754Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Base de dados a ser treinada\n",
    "x = pd.DataFrame(\n",
    "    [[1],\n",
    "    [2],\n",
    "    [3]],\n",
    "    columns=['valores x'])\n",
    "\n",
    "d = pd.DataFrame(\n",
    "    [[5],\n",
    "    [4],\n",
    "    [3]],\n",
    "    columns=['valores desejados'])\n",
    "\n",
    "# Convertendo o dataframe em array e normalizando os valores desejados para ficar entre 0 e +1.\n",
    "x = x.to_numpy()\n",
    "d = d.to_numpy()\n",
    "\n",
    "\n",
    "def df(x):\n",
    "    return(np.where(np.array(x) <= 0, 0.01, 1))\n",
    "\n",
    "def f(x):\n",
    "    return(np.where(np.array(x) > 0, x, x * 0.01))\n",
    "\n",
    "\n",
    "#def rede_mlp(n, x, d, net, k, precisao):\n",
    "\n",
    "# Construindo a rede de duas camadas \n",
    "# net = número de neurônios na primeira camada\n",
    "# n = taxa de aprendizagem\n",
    "# precisao = precisão do erro quadrático médio\n",
    "net=3\n",
    "n = 1e-3\n",
    "precisao=1e-5\n",
    "w1 = np.random.rand(net,len(x[0]))\n",
    "bias1 = np.random.rand()\n",
    "\n",
    "w2 = np.random.rand(1,net)\n",
    "bias2 = np.random.rand()\n",
    "\n",
    "E_M=20\n",
    "epocas=0\n",
    "\n",
    "while E_M>precisao:\n",
    "    E_M=0\n",
    "    errofinal=0\n",
    "    for i in range(0,len(x)):\n",
    "\n",
    "        # FOWARD\n",
    "        i1 = np.matmul(w1, x[i].reshape(len(x[i]),1)) + bias1\n",
    "        y1 = f(i1)\n",
    "\n",
    "\n",
    "        i2 = np.matmul(w2, y1) + bias2\n",
    "        y2 = f(i2)\n",
    "\n",
    "\n",
    "        # erro com o valor desejado\n",
    "        erro = d[i].reshape(len(d[i]),1) - y2\n",
    "\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        delta_2 = erro*df(i2)\n",
    "        delta_1 = (np.matmul(w2.T, delta_2))*df(i1)\n",
    "        bias2 += n * delta_2\n",
    "        bias1 += n * delta_1\n",
    "        \n",
    "        w2 = w2 + n*(np.matmul(delta_2, y1.reshape(1, net)))\n",
    "        w1 = w1 + n*(np.matmul(delta_1, x[i].reshape(1, len(x[i]))))\n",
    "\n",
    "\n",
    "        errofinal = errofinal + 0.5*erro**2\n",
    "\n",
    "    #E_M = errofinal/len(x)\n",
    "    E_M = errofinal\n",
    "    epocas+=1\n",
    "    print(E_M)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
